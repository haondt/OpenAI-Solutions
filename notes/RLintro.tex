\documentclass[12pt]{article}
\nonstopmode
\usepackage[a4paper, left=1in, top=1in, right=1in, bottom=1in]{geometry}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
 \lstdefinestyle{mystyle}{
	     backgroundcolor=\color{backcolour},
		 numberstyle=\tiny\color{codegray},
		 numbers=left,
		 escapeinside={(*}{*)},
		 tabsize=2,
		 basicstyle=\linespread{1.1}\ttfamily,
		 xleftmargin-0.7cm,
		 frame=tlbr,
		 framesep=0.2cm,
		 framerule=0pt
 }
		 


\lstset{style=mystyle}
\title{Notes}
\begin{document}
\maketitle
A collection of notes derived from the book, 
\textit{Reinforcement Learning} by Richard Sutton and Andrew Barto.
Available at \url{http://incompleteideas.net/book/the-book.html}

\section{Action selection}
Most RL methods require some form of policy or action-value based action selection method.
\begin{itemize}
\item\textbf{Greedy Selection}: Choosing the best action.
\[A = \text{argmax}_aQ(a) \]

\item$\boldsymbol\varepsilon$\textbf{-greedy Selection}: Simple exploration with $\varepsilon$-probability.
\[
	A\leftarrow
	\begin{cases}
		\text{argmax}_aQ(a) & \text{with probability 1 - $\varepsilon$ (breaking ties randomly)} \\
		\text{a random action} & \text{with probability } \varepsilon
	\end{cases}
\]
\item\textbf{Upper Confidence Bound (UCB)}: Takes into account the proximity of the estimate to being maximal and the uncertanty in the estimates. Does not perform well on large state spaces.
\[
	A_t=\underset{a}{\text{argmax}} 
	\left[
		Q_t(a) + c \sqrt{\frac{\ln{t}}{N_t(a)}}\;
	\right]
\]
Where:
	\begin{itemize}
		\item $c > 0$ is the degree of exploration
		\item $N_t(a)$ is the number of times that action $a$ has been selected prior to time $t$.
			If $N_t(a)=0$, then $a$ is considered to be a maximizing action.
	\end{itemize}
\end{itemize}

\section{Performance Measures}
Methods for comparing the performance of different parameters and algorithms.
\begin{itemize}
\item\textbf{Optimal Action \%}: Requires knowledge of the workings of the environment 
	and whether the action was optimal. Plot \% over steps.

\item\textbf{Average reward}: Simply plot the average reward over steps.
	Good for comparing specific implementation of agent in specific implementation of environment.

\item\textbf{Average reward w.r.t. parameter}: Plot the average reward over first n=1000 steps 
	against input parameter(s) ($\varepsilon$, $\alpha$, $c$, $Q_0$ etc) on a logarithmic scale.
	Good for comparing learning algorithms' general effectiveness and finding the best parameter value.

\item\textbf{Mean Square Error}: Plot the mean square error (averaged over n=100 runs) of the value of 
	a single state (error = actual-estimate) over the number of episodes run before acheiving the 
	estimate, with the episodes on a logarithmic scale. Good for Monte Carlo method, where you can form 
	an estimate of a single state without forming an estimate of the others.
\end{itemize}

\section{Algorithms}
Reinforcement learning algorithms covered by the book.
\begin{itemize}
	\item\textbf{Dynamic Programming / Value Iteration}: Updating state values by sweeping through all states.
		Computationally expensive, especially on large state spaces.
	\begin{lstlisting}
Parameters:
	a small threshold (*$\theta$*) > 0 
	Initialize (*$V(s) \;\; \forall s \in S^+$*) arbitrarily, except that
	(*$V(terminal)=0$*) 

Loop:
	(*$\Delta\leftarrow 0$*)
	Loop for each (*$s \in S^+$*):
		(*$v\leftarrow V(s)$*)
		(*$V(s) \leftarrow \text{max}_a \sum_{s',r} p(s',r|s,a)\left[r+\gamma V(s')\right]$*)
		(*$\Delta\leftarrow\text{max}(\Delta,|v-V(s)|)$*)
until (*$\Delta < \theta$*)

Output a deterministic policy (*$\pi$*), such that
	(*$\pi (s) = \text{argmax}_a \sum_{s',r}p(s',r|s,a)\left[ r+\gamma V(s')\right]$*)
		\end{lstlisting}

	\item\textbf{Off-policy Monte Carlo control}: Using a soft policy to explore, while estimating the
		optimal policy. Does not bootstrap, i.e. doesn't use estimates of previous states to estimate the
		current state value.
			\begin{lstlisting}
Parameters:
	For all (*$s \in S, a \in A(s)$*):
		Initialize (*$Q(s,a)$*) arbitrarily
		(*$C(s,a) \leftarrow 0$*)
		(*$\pi(s) \leftarrow \text{argmax}_aQ(s,a)$*) (with ties broken consistently)
Loop for each episode:
	(*$b \leftarrow$*)any soft policy
	Generate an episode using(*$b\text{:}S_0, A_0, R_1, \ldots, S_{T-1}, A_{T-1}, R_T$*)
	(*$G \leftarrow 0 $*)
	(*$W \leftarrow 1 $*)
	Loop for each step of episode,(*$ t = T-1, T-2, \ldots, 0$*):
		(*$G\leftarrow \gamma G + R_{t+1}$*)
		(*$C(S_t,A_t) \leftarrow C(S_t, A_t) + W$*)
		(*$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \frac{W}{C(S_t,A_t)} \left[ G-Q(S_t,A_t) \right]$*)
		(*$\pi (S_t) \leftarrow \text{argmax}_a Q(S_t,a)$*) (with ties broken consistently)
		If (*$A_t \neq \pi (S_t)$*), exit inner Loop (proceed to next episode)
		(*$W \leftarrow W\frac{1}{b(A_t|S_t)}$*)
			\end{lstlisting}


\end{itemize}

\end{document}
