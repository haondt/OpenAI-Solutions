# Noah Burghardt 2018/11/17
# Agent for a 1000-step random walk environment
# TD(0) with Tile coding features
# uses the tiles software via
# http://www.incompleteideas.net/tiles/tiles3.html
import numpy as np
from tiles3 import *

class Agent:
	"""
	Defines the interface of an RLGlue Agent

	ie. These methods must be defined in your own Agent classes
	"""
	# feature and weight vectors
	# states are indexed [1-1000]
	# left terminal state at i=0
	# right terminal state at i=1001
	iht = None
	w = None
	z = None
	numTilings = None
	stepSize = None
	tilecache = None
	maxSize = None
	lam = None
	epsilon = None
	gamma = None
	# decelerate, coast, accelerate
	actions = [0,1,2]

	# number of states
	oldstate = None
	oldaction = None

	def __init__(self):
		"""Declare agent variables."""
	def agent_init(self):
		"""
		run once, in experiment
		Initialize agent variables.
		"""
		self.maxSize = 2048
		self.iht = IHT(self.maxSize)
		# initialize weights to random numbers between 0 and -0.001
		self.w = -0.001*np.random.ranf(self.maxSize)
		self.numTilings = 8
		self.stepSize = 0.559184/self.numTilings
		self.tilecache = {}
		self.lam = 0.9
		self.epsilon = 0.0
		self.gamma = 1

	def agent_start(self, state):
		"""
		run at the beginning of an episode
		The first method called when the experiment starts, called after
		the environment starts.
		Args:
			state (state observation): The agent's current state

		Returns:
			The first action the agent takes.
		"""

		# initialize trace
		self.z = np.zeros(self.maxSize)
		self.tilecache = {}

		# save state
		self.oldstate = state
		
		# choose action
		action = self.epGreedy(state)
		self.oldaction = action
		
		return action
	
	def agent_step(self, reward, state):
		"""
		A step taken by the agent.
		Args:
		for tile in tiles:
			estimate += self.w[tile]
			reward (float): the reward received for taking the last action taken
			state (state observation): The agent's current state
		Returns:
			The action the agent is taking.
		"""
		# compute A' in S'
		action = self.epGreedy(state)

		# compute td error (action value form)
		delta = reward + self.gamma*self.Q(state,action) - self.Q(self.oldstate, self.oldaction)

		# update trace for S, A
		self.z[self.x(self.oldstate,self.oldaction)] = 1

		# update weights with TD-error and trace
		self.w += self.stepSize*delta*self.z
		self.z *= self.gamma*self.lam


		# update state and action
		self.oldstate = state
		self.oldaction = action


		return action


	def agent_end(self, reward):
		"""
		Run when the agent terminates.
		Args:
			reward (float): the reward the agent received for entering the
				terminal state.
		"""
		
		# compute td error (action value form)
		delta = reward - self.Q(self.oldstate, self.oldaction)
		
		# update trace for S, A
		self.z[self.x(self.oldstate,self.oldaction)] = 1
		
		# update weights with TD-error and trace
		self.w += self.stepSize*delta*self.z

	# returns data to be passed to 3dplot.py
	def agent_message(self, message):
		"""
		receive a message from rlglue
		args:
			message (str): the message passed
		returns:
			str : the agent's response to the message (optional)
		"""

		# allow step size to be changed on the fly
		if "step" in message:
			message = message.split(':')
			self.stepSize = float(message[1])/self.numTilings

		elif message == 'graph':
			resolution = 50
			# get np array for (50) position values
			x = np.linspace(-1.2, 0.5, resolution)

			# get np array for (50) velocity values
			y = np.linspace(-0.07, 0.07, resolution)
			# convert x,y to mesh grid
			x, y = np.meshgrid(x, y)
			
			z = np.zeros((resolution, resolution))

			for i in range(resolution):
				for j in range(resolution):
					vals = [self.Q((x[i][j],y[i][j]), action) for action in self.actions]
					z[j][-1-i] = -max(vals)
			# get np array for maximum state action values
			return x, y, z
	# choose action according to self.epsilon
	def epGreedy(self, state):
		if np.random.random() > self.epsilon:	# prob 1-epsilon
			return max(self.actions, key=lambda x: self.Q(state, x))
		else:	# prob epsilon
			return np.random.choice(self.actions)
		
	
	
	# computes action value from given state
	def Q(self, state, action, w=None):
		if w == None:
			w = self.w
		features = self.x(state,action)
		return np.sum(self.w[features])
	
	# returns array of feature vector 1's 
	def x(self, state, action):
		# state is a tuple of position and velocity
		# pos is from -1.2 to 0.5
		# velocity is from -0.07 to 0.07
		position, velocity = state
		if (tuple(state), action) in self.tilecache:
			return self.tilecache[(tuple(state),action)]
		else:
			# scale factor generated by numtiles / upper bound -lower bound
			posScaleFactor = self.numTilings/(0.5+1.2)
			velScaleFactor = self.numTilings/(0.07+0.07)
			t = np.array(tiles(self.iht, self.numTilings, [posScaleFactor*position, velScaleFactor*velocity], [action]))
			self.tilecache[(tuple(state),action)] = t
			return t
	

