\documentclass[12pt]{article}
\nonstopmode
\usepackage[a4paper, left=1in, top=1in, right=1in, bottom=1in]{geometry}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
 \lstdefinestyle{mystyle}{
	     backgroundcolor=\color{backcolour},
		 numberstyle=\tiny\color{codegray},
		 numbers=left,
		 escapeinside={(*}{*)},
		 tabsize=2,
		 basicstyle=\linespread{1.1}\ttfamily,
		 xleftmargin-0.7cm,
		 frame=tlbr,
		 framesep=0.2cm,
		 framerule=0pt
 }
		 


\lstset{style=mystyle}
\title{Notes}
\begin{document}
\maketitle
A collection of notes from the textbook, 
\textit{Reinforcement Learning} by Richard Sutton and Andrew Barto.
Available at \url{http://incompleteideas.net/book/the-book.html}

\section{Action selection}
Most RL methods require some form of policy or action-value based action selection alogrithm.
\begin{itemize}
\item\textbf{Greedy Selection}: Choosing the best action.
\[A = \text{argmax}_aQ(a) \]

\item$\boldsymbol\varepsilon$\textbf{-greedy Selection}: Simple exploration with $\varepsilon$-probability.
\[
	A\leftarrow
	\begin{cases}
		\text{argmax}_aQ(a) & \text{with probability 1 - $\varepsilon$ (breaking ties randomly)} \\
		\text{a random action} & \text{with probability } \varepsilon
	\end{cases}
\]
\item\textbf{Upper Confidence Bound (UCB)}: Takes into account the proximity of the estimate to being maximal and the uncertanty in the estimates. Does not perform well on large state spaces.
\[
	A_t=\underset{a}{\text{argmax}} 
	\left[
		Q_t(a) + c \sqrt{\frac{\ln{t}}{N_t(a)}}\;
	\right]
\]
Where:
	\begin{itemize}
		\item $c > 0$ is the degree of exploration
		\item $N_t(a)$ is the number of times that action $a$ has been selected prior to time $t$.
			If $N_t(a)=0$, then $a$ is considered to be a maximizing action.
	\end{itemize}
\end{itemize}

\section{Performance Measures}
\begin{itemize}
\item\textbf{Optimal Action \%}: Requires knowledge of the workings of the environment 
	and whether the action was optimal. Plot \% over steps.

\item\textbf{Average reward}: Simply plot the average reward over steps.
	Good for comparing specific implementation of agent in specific implementation of environment.

\item\textbf{Average reward w.r.t. parameter}: Plot the average reward over first n=1000 steps 
	against input parameter(s) ($\varepsilon$, $\alpha$, $c$, $Q_0$ etc) on a logarithmic scale.
	Good for comparing learning algorithms' general effectiveness and finding the best parameter value.

\end{itemize}

\section{Algorithms}
\begin{itemize}
	\item\textbf{Dynamic Programming / Value Iteration}: Updating state values by sweeping through all states.
		Computationally expensive, especially on large state spaces.
		\begin{lstlisting}
Parameters:
	a small threshold (*$\theta$*) > 0 
	Initialize (*$V(s) \;\; \forall s \in S^+$*) arbitrarily, except that
	(*$V(terminal)=0$*) 

Loop:
	(*$\Delta\leftarrow 0$*)
	Loop for each (*$s \in S^+$*):
		(*$v\leftarrow V(s)$*)
		(*$V(s) \leftarrow \text{max}_a \sum_{s',r} p(s',r|s,a)\left[r+\gamma V(s')\right]$*)
		(*$\Delta\leftarrow\text{max}(\Delta,|v-V(s)|)$*)
until (*$\Delta < \theta$*)

Output a deterministic policy (*$\pi$*), such that
	(*$\pi (s) = \text{argmax}_a \sum_{s',r}p(s',r|s,a)\left[ r+\gamma V(s')\right]$*)
		\end{lstlisting}
\end{itemize}

\end{document}
